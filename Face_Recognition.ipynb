{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf075ef-8439-40e8-8d77-00ee7cb43b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: face_recognition in c:\\users\\user\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: face-recognition-models>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from face_recognition) (0.3.0)\n",
      "Requirement already satisfied: Click>=6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from face_recognition) (8.1.7)\n",
      "Requirement already satisfied: dlib>=19.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from face_recognition) (19.24.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from face_recognition) (1.26.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\user\\anaconda3\\lib\\site-packages (from face_recognition) (10.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from Click>=6.0->face_recognition) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4839a6b6-7d28-4526-8cd3-9284b79f5d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\user\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1726433-00d2-43b8-b8c6-a26ad37e9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition #Recognize faces and compare it with the faces already in the database\n",
    "import cv2 #get input from webcam(opencv) and process it and give it to the face_recognition\n",
    "import numpy as np\n",
    "import csv #Handle the csv file \n",
    "import os # package to access the files(if have 100 of photos you can automate by using os)\n",
    "import glob\n",
    "from datetime import datetime #Get exact date and time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d36b470-183d-4514-a005-f454ad929b0f",
   "metadata": {},
   "source": [
    "video capture method from opencv\n",
    "put 0 because we are taking the input from the default webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bfca327-d815-44a4-8b33-b3b2f3f8cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture=cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aa58539-e6ae-4eba-afeb-6931a9231d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bill_image = face_recognition.load_image_file(\"photos/Bill.jpg\")\n",
    "Bill_encoding = face_recognition.face_encodings(Bill_image)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c118ba07-da90-4c01-bc42-f116766fb859",
   "metadata": {},
   "outputs": [],
   "source": [
    "Elon_image=face_recognition.load_image_file(\"photos/Elon.jpg\")\n",
    "Elon_encoding=face_recognition.face_encodings(Elon_image)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea60035-b437-4132-8234-1233552a1ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mark_image=face_recognition.load_image_file(\"photos/Mark.jpg\")\n",
    "Mark_encoding=face_recognition.face_encodings(Mark_image)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a290a846-15eb-4358-964c-204929099692",
   "metadata": {},
   "outputs": [],
   "source": [
    "Steve_image=face_recognition.load_image_file(\"photos/Steve.jpg\")\n",
    "Steve_encoding=face_recognition.face_encodings(Steve_image)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d8b8246-db9f-4840-991b-b09dc8258d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_face_encoding = [\n",
    "    Bill_encoding,  # Corresponds to \"Bill\"\n",
    "    Elon_encoding,  # Corresponds to \"Elon\"\n",
    "    Mark_encoding,  # Corresponds to \"Mark\"\n",
    "    Steve_encoding  # Corresponds to \"Steve\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19aa031d-c8e9-4918-ac66-baeb7897e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_faces_names = [\n",
    "    \"Bill\",  # Matches the first encoding\n",
    "    \"Elon\",  # Matches the second encoding\n",
    "    \"Mark\",  # Matches the third encoding\n",
    "    \"Steve\"  # Matches the fourth encoding\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6adc818-82e1-499d-a143-de12393e8ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "students = known_faces_names.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab2565aa-53b8-4d43-b46f-b0fc83c5a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "face_locations = []# to save face locations\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "s = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92fe7c16-06dc-4427-a359-4005eaedb638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the current date to \"YYYY-MM-DD\"\n",
    "current_date = now.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f80cc491-de72-4551-ae72-7bc8761bd41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(current_date + '.csv', 'w+', newline='')# Open or create a file with the current date as the filename\n",
    "lnwriter = csv.writer(f) # Initialize CSV writer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f282ba63-648c-483b-9be5-3631a08f9162",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: list[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x00000241ABEC1270>, array([[[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [ 13,  12,  13],\n        [ 18,  15,  17],\n        [ 27,  27,  32]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [ 11,  11,  11],\n        [ 18,  18,  19],\n        [ 29,  27,  32]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [ 14,  15,  14],\n        [ 17,  15,  19],\n        [ 26,  25,  31]],\n\n       ...,\n\n       [[ 58,  42,  37],\n        [172, 143, 147],\n        [211, 195, 197],\n        ...,\n        [  6,   7,  11],\n        [  7,   5,   7],\n        [  4,   4,   5]],\n\n       [[117,  88,  93],\n        [180, 159, 154],\n        [179, 142, 143],\n        ...,\n        [  8,   8,   8],\n        [  9,  11,  12],\n        [  7,   9,  10]],\n\n       [[135,  95, 105],\n        [154, 127, 128],\n        [131, 100,  91],\n        ...,\n        [  6,   6,   6],\n        [  9,   9,   9],\n        [  8,   8,  10]]], dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x00000241B89C8DB0>, 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m rgb_small_frame \u001b[38;5;241m=\u001b[39m small_frame[:, :, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Convert BGR to RGB\u001b[39;00m\n\u001b[0;32m      7\u001b[0m face_locations \u001b[38;5;241m=\u001b[39m face_recognition\u001b[38;5;241m.\u001b[39mface_locations(rgb_small_frame)\n\u001b[1;32m----> 8\u001b[0m face_encodings \u001b[38;5;241m=\u001b[39m face_recognition\u001b[38;5;241m.\u001b[39mface_encodings(rgb_small_frame, face_locations)\n\u001b[0;32m     10\u001b[0m face_names \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m face_encoding \u001b[38;5;129;01min\u001b[39;00m face_encodings:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\face_recognition\\api.py:214\u001b[0m, in \u001b[0;36mface_encodings\u001b[1;34m(face_image, known_face_locations, num_jitters, model)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03mGiven an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m:return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m raw_landmarks \u001b[38;5;241m=\u001b[39m _raw_face_landmarks(face_image, known_face_locations, model)\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39marray(face_encoder\u001b[38;5;241m.\u001b[39mcompute_face_descriptor(face_image, raw_landmark_set, num_jitters)) \u001b[38;5;28;01mfor\u001b[39;00m raw_landmark_set \u001b[38;5;129;01min\u001b[39;00m raw_landmarks]\n",
      "\u001b[1;31mTypeError\u001b[0m: compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: list[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x00000241ABEC1270>, array([[[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [ 13,  12,  13],\n        [ 18,  15,  17],\n        [ 27,  27,  32]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [ 11,  11,  11],\n        [ 18,  18,  19],\n        [ 29,  27,  32]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [ 14,  15,  14],\n        [ 17,  15,  19],\n        [ 26,  25,  31]],\n\n       ...,\n\n       [[ 58,  42,  37],\n        [172, 143, 147],\n        [211, 195, 197],\n        ...,\n        [  6,   7,  11],\n        [  7,   5,   7],\n        [  4,   4,   5]],\n\n       [[117,  88,  93],\n        [180, 159, 154],\n        [179, 142, 143],\n        ...,\n        [  8,   8,   8],\n        [  9,  11,  12],\n        [  7,   9,  10]],\n\n       [[135,  95, 105],\n        [154, 127, 128],\n        [131, 100,  91],\n        ...,\n        [  6,   6,   6],\n        [  9,   9,   9],\n        [  8,   8,  10]]], dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x00000241B89C8DB0>, 1"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    _, frame = video_capture.read()\n",
    "    # Resize frame for faster processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "    rgb_small_frame = small_frame[:, :, ::-1]  # Convert BGR to RGB\n",
    "\n",
    "    face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "    face_names = []\n",
    "    for face_encoding in face_encodings:\n",
    "        matches = face_recognition.compare_faces(known_face_encoding, face_encoding)\n",
    "        face_distance = face_recognition.face_distance(known_face_encoding, face_encoding)\n",
    "        best_match_index = np.argmin(face_distance)\n",
    "        if matches[best_match_index]:\n",
    "            name = known_faces_names[best_match_index]\n",
    "            face_names.append(name)\n",
    "\n",
    "        if name in students:  # Check if the student is in the students list\n",
    "            students.remove(name)  # Remove the student after marking them present\n",
    "            print(students)\n",
    "\n",
    "            # Record attendance\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            lnwriter.writerow([name, current_time])\n",
    "\n",
    "    # Display video feed\n",
    "    cv2.imshow(\"attendance system\", frame)\n",
    "\n",
    "    # Break loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture and close all windows\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a06d76a-8d67-472d-bd0f-d37f357297d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf14a24-4fe8-4249-828c-52af93193b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8134b3-d038-4321-a0e6-11d71c6266ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a1525e-1fc5-4dc3-9cb4-a88ed46b9773",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    _, frame = video_capture.read()\n",
    "    # Resize frame for faster processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "    rgb_small_frame = small_frame[:, :, ::-1]  # Convert BGR to RGB\n",
    "\n",
    "    face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "    face_names = []\n",
    "    for face_encoding in face_encodings:\n",
    "        matches = face_recognition.compare_faces(known_face_encoding, face_encoding)\n",
    "        face_distance = face_recognition.face_distance(known_face_encoding, face_encoding)\n",
    "        best_match_index = np.argmin(face_distance)\n",
    "        if matches[best_match_index]:\n",
    "            name = known_faces_names[best_match_index]\n",
    "            face_names.append(name)\n",
    "\n",
    "        if name in students:  # Check if the student is in the students list\n",
    "            students.remove(name)  # Remove the student after marking them present\n",
    "            print(students)\n",
    "\n",
    "            # Record attendance\n",
    "            current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            lnwriter.writerow([name, current_time])\n",
    "\n",
    "    # Display video feed\n",
    "    cv2.imshow(\"attendance system\", frame)\n",
    "\n",
    "    # Break loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture and close all windows\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5858adb4-3bc9-4174-b1e9-5c45eea4444a",
   "metadata": {},
   "outputs": [],
   "source": [
    " if name in known_faces_names:\n",
    "                print(f\"Recognized: {name}\")  # Debugging: Print recognized name\n",
    "                \n",
    "                if name in students:\n",
    "                    print(f\"Before removing {name}: {students}\")  # Debugging: Print students list before removal\n",
    "                    students.remove(name)  # Remove the student to mark attendance\n",
    "                    print(f\"After removing {name}: {students}\")  # Debugging: Print students list after removal\n",
    "                    \n",
    "                    # Log the name and current time to the CSV file\n",
    "                    current_time = now.strftime(\"%H:%M:%S\")\n",
    "                    lnwriter.writerow([name, current_time])\n",
    "                    f.flush()  # Ensure data is written to the file immediately\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
